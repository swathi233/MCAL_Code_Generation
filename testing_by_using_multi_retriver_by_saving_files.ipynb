{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement some_library (from versions: none)\n",
      "ERROR: No matching distribution found for some_library\n"
     ]
    }
   ],
   "source": [
    "# ! pip install PyPDF2\n",
    "# ! pip uninstall fitz\n",
    "# ! pip install pymupdf\n",
    "# ! pip install tabula\n",
    "# ! pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HTW1KOR\\Documents\\hackathon\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\HTW1KOR\\Documents\\hackathon\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "Specification of GPT Driver\n",
      "AUTOSAR CP R23-11\n",
      "The function Gpt_StartTimer shall raise the error GPT_E_PARAM_VALUE if the pa-\n",
      "rameter Value is \"0\" or not within the allowed range (exceeding the maximum timer\n",
      "resolution).⌋(SRS_BSW_00323)\n",
      "[SWS_Gpt_00224] ⌈If development error detection is enabled for GPT module:\n",
      "If the driver is not initialized, the function Gpt_StartTimer shall raise the error GPT_-\n",
      "E_UNINIT .⌋(SRS_BSW_00406)\n",
      "[SWS_Gpt_00084] ⌈If the function Gpt_StartTimer is called on a Channel in state\n",
      "\"running\", the function shall raise the runtime error GPT_E_BUSY .⌋()\n",
      "8.3.7 Gpt_StopTimer\n",
      "[SWS_Gpt_00285] Definition of API function Gpt_StopTimer ⌈\n",
      "Service Name Gpt_StopTimer\n",
      "Syntax void Gpt_StopTimer (\n",
      "Gpt_ChannelType Channel\n",
      ")\n",
      "Service ID [hex] 0x06\n",
      "Sync/Async Synchronous\n",
      "Reentrancy Reentrant (but not for the same timer channel)\n",
      "Parameters (in) Channel Numeric identifier of the GPT channel.\n",
      "Parameters (inout) None\n",
      "Parameters (out) None\n",
      "Return value None\n",
      "Description Stops a timer channel.\n",
      "Available via Gpt.h\n",
      "⌋(SRS_Gpt_12119)\n",
      "[SWS_Gpt_00013] ⌈The function Gpt_StopTimer shall stop the selected timer\n",
      "Channel .⌋(SRS_Gpt_12119)\n",
      "[SWS_Gpt_00343] ⌈The state of the selected timer Channel shall be changed to\n",
      "\"stopped\" if Gpt_StopTimer is called.⌋()\n",
      "[SWS_Gpt_00099] ⌈If development error detection is enabled for GPT module:\n",
      "If the function Gpt_StopTimer is called on a Channel in state \"initialized\", \"stopped\"\n",
      "or \"expired\", the function shall not raise a development error. ⌋()\n",
      "[SWS_Gpt_00344] ⌈If the function Gpt_StopTimer is called on a Channel in state\n",
      "\"initialized\", \"stopped\" or \"expired\", the function shall leave without any action (no\n",
      "change of the Channel state).⌋()\n",
      "[SWS_Gpt_00116] ⌈The function Gpt_StopTimer shall be reentrant, if the timer\n",
      "Channel s used in concurrent calls are different. ⌋()\n",
      "[SWS_Gpt_00213] ⌈If development error detection is enabled for GPT module:\n",
      "32 of 63 Document ID 30: AUTOSAR_CP_SWS_GPTDriver\n",
      "\n",
      "Table 1:\n",
      "<table><tr><td>Gpt_StartTimer(Gpt_ChannelType,</td></tr><tr><td>Gpt_ValueType)</td></tr></table>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Function to convert table to text\n",
    "def table_to_text(table):\n",
    "    return \"\\n\".join([\"\\t\".join(map(str, row)) for row in table])\n",
    "\n",
    "# Function to convert HTML table to plain text\n",
    "def html_to_plain_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text(separator=\"\\n\")\n",
    "\n",
    "# Function to convert text to HTML table\n",
    "def text_to_html_table(table_text):\n",
    "    rows = table_text.split(\"\\n\")\n",
    "    soup = BeautifulSoup(\"<table></table>\", \"html.parser\")\n",
    "    table_tag = soup.table\n",
    "    for row in rows:\n",
    "        row_tag = soup.new_tag(\"tr\")\n",
    "        table_tag.append(row_tag)\n",
    "        for cell in row.split(\"\\t\"):\n",
    "            cell_tag = soup.new_tag(\"td\")\n",
    "            cell_tag.string = cell\n",
    "            row_tag.append(cell_tag)\n",
    "    return str(table_tag)\n",
    "\n",
    "# Load and split text from PDF\n",
    "def load_and_split_pdf_text(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    text = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(text)\n",
    "\n",
    "# Extract tables from PDF\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        tables = []\n",
    "        for page in pdf.pages:\n",
    "            tables.extend(page.extract_tables())\n",
    "    all_tables_text = [table_to_text(table) for table in tables if table]\n",
    "    return [text_to_html_table(table_text) for table_text in all_tables_text]\n",
    "\n",
    "# Load text from both PDFs\n",
    "pdf1_text_splits = load_and_split_pdf_text('AUTOSAR_CP_SWS_GPTDriver.pdf')  # Generic AUTOSAR document\n",
    "pdf2_text_splits = load_and_split_pdf_text('path_of_the_pdf')\n",
    "\n",
    "# Extract tables from both PDFs\n",
    "pdf1_html_tables = extract_tables_from_pdf('AUTOSAR_CP_SWS_GPTDriver.pdf')  # # Generic AUTOSAR document\n",
    "pdf2_html_tables = extract_tables_from_pdf('path_of_the_pdf')\n",
    "\n",
    "# Combine text splits and tables\n",
    "all_text_splits = pdf1_text_splits + pdf2_text_splits\n",
    "all_html_tables = pdf1_html_tables + pdf2_html_tables\n",
    "\n",
    "# Embed text\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_embeddings = model.encode([split.page_content for split in all_text_splits])\n",
    "table_plain_texts = [html_to_plain_text(html_table) for html_table in all_html_tables]\n",
    "table_embeddings = model.encode(table_plain_texts)\n",
    "\n",
    "# Store text embeddings in FAISS\n",
    "dimension = text_embeddings.shape[1]\n",
    "text_index = faiss.IndexFlatL2(dimension)\n",
    "text_index.add(np.array(text_embeddings))\n",
    "\n",
    "# Store table embeddings in FAISS\n",
    "dimension = table_embeddings.shape[1]\n",
    "table_index = faiss.IndexFlatL2(dimension)\n",
    "table_index.add(np.array(table_embeddings))\n",
    "\n",
    "# Implement a multi-vector retriever\n",
    "class MultiVectorRetriever:\n",
    "    def __init__(self, text_index, table_index, model):\n",
    "        self.text_index = text_index\n",
    "        self.table_index = table_index\n",
    "        self.model = model\n",
    "    \n",
    "    def retrieve(self, query, k=1):\n",
    "        query_embedding = self.model.encode([query])\n",
    "        text_distances, text_indices = self.text_index.search(query_embedding, k)\n",
    "        table_distances, table_indices = self.table_index.search(query_embedding, k)\n",
    "        \n",
    "        retrieved_texts = [all_text_splits[idx].page_content for idx in text_indices[0]]\n",
    "        retrieved_tables = [all_html_tables[idx] for idx in table_indices[0]]\n",
    "        \n",
    "        return retrieved_texts, retrieved_tables\n",
    "\n",
    "retriever = MultiVectorRetriever(text_index, table_index, model)\n",
    "\n",
    "# Query the retriever\n",
    "query = \"Gpt_StartTimer?\"\n",
    "retrieved_texts, retrieved_tables = retriever.retrieve(query)\n",
    "\n",
    "for i, text in enumerate(retrieved_texts):\n",
    "    print(f\"Text {i+1}:\\n{text}\\n\")\n",
    "\n",
    "for i, table in enumerate(retrieved_tables):\n",
    "    print(f\"Table {i+1}:\\n{table}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.1.42\n"
     ]
    }
   ],
   "source": [
    "! ollama -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            \tID          \tSIZE  \tMODIFIED   \n",
      "codellama:latest\t8fdf8f752f6e\t3.8 GB\t2 days ago\t\n",
      "codegemma:latest\t0c96700aaada\t5.0 GB\t2 days ago\t\n"
     ]
    }
   ],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 10028\n",
      "Generated code saved to CAN_driver_code.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Define the function to handle retriever queries (dummy function for now)\n",
    "def retrieve_query(query):\n",
    "    # Dummy function to simulate retrieval\n",
    "    if \"GPT\" in query:\n",
    "        return {\"retrieved_texts\": [\"Dummy retrieval for \" + query], \"retrieved_tables\": []}\n",
    "    else:\n",
    "        # Implement retrieval logic for other queries\n",
    "        pass\n",
    "\n",
    "# Define the few-shot learning examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"query\": \"Generate C and H code for GPIO initialization on STM32F4\",\n",
    "        \"desired_output\": {\n",
    "            \"c_code\": \"\"\"\n",
    "                #include \"stm32f4_gpio.h\"\n",
    "                \n",
    "                void GPIO_Init(GPIO_TypeDef* GPIOx, GPIO_InitTypeDef* GPIO_InitStruct) {\n",
    "                    // Implementation for GPIO initialization\n",
    "                }\n",
    "                \"\"\",\n",
    "            \"h_code\": \"\"\"\n",
    "                #ifndef STM32F4_GPIO_H\n",
    "                #define STM32F4_GPIO_H\n",
    "                \n",
    "                // Header file content for GPIO initialization\n",
    "                \n",
    "                #endif // STM32F4_GPIO_H\n",
    "                \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Generate C code for GPT driver\",\n",
    "        \"desired_output\": {\n",
    "            \"c_code\": \"\"\"\n",
    "                // C code for GPT driver implementation\n",
    "                // Include necessary headers and define functions\n",
    "                \"\"\",\n",
    "            \"h_code\": \"\"\"\n",
    "                // Header file for GPT driver\n",
    "                // Define data structures and function prototypes\n",
    "                \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Generate C and H code for PORT driver\",\n",
    "        \"desired_output\": {\n",
    "            \"c_code\": \"\"\"\n",
    "                // C code for PORT driver implementation\n",
    "                // Include necessary headers and define functions\n",
    "                \"\"\",\n",
    "            \"h_code\": \"\"\"\n",
    "                // Header file for PORT driver\n",
    "                // Define data structures and function prototypes\n",
    "                \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Generate C and H code for SPI driver\",\n",
    "        \"desired_output\": {\n",
    "            \"c_code\": \"\"\"\n",
    "                // C code for SPI driver implementation\n",
    "                // Include necessary headers and define functions\n",
    "                \"\"\",\n",
    "            \"h_code\": \"\"\"\n",
    "                // Header file for SPI driver\n",
    "                // Define data structures and function prototypes\n",
    "                \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Generate C and H code for CAN driver\",\n",
    "        \"desired_output\": {\n",
    "            \"c_code\": \"\"\"\n",
    "                // C code for CAN driver implementation\n",
    "                // Include necessary headers and define functions\n",
    "                \"\"\",\n",
    "            \"h_code\": \"\"\"\n",
    "                // Header file for CAN driver\n",
    "                // Define data structures and function prototypes\n",
    "                \"\"\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define a function to retrieve desired output based on query\n",
    "def retrieve_desired_output(query):\n",
    "    for example in few_shot_examples:\n",
    "        if example[\"query\"].lower() == query.lower():\n",
    "            # Retrieve desired output\n",
    "            desired_output = example[\"desired_output\"]\n",
    "            \n",
    "            # Filter out non-header lines from the .h code\n",
    "            h_code_lines = desired_output[\"h_code\"].split('\\n')\n",
    "            filtered_h_code_lines = [line for line in h_code_lines if line.strip().startswith('#') or line.strip().startswith('typedef') or line.strip().startswith('struct')]\n",
    "            desired_output[\"h_code\"] = '\\n'.join(filtered_h_code_lines)\n",
    "            \n",
    "            return desired_output\n",
    "    return None\n",
    "\n",
    "# Start ollama as a background process with interactive mode enabled\n",
    "command = \"nohup ollama serve --interactive &\"\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"Process ID:\", process.pid)\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Local LLM\n",
    "ollama_llm = \"codellama\"\n",
    "model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "# Define the processing chain\n",
    "chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_query), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to generate C and H code based on user query\n",
    "def generate_mcal_code(query):\n",
    "    retrieved_output = chain.invoke(query)\n",
    "    return retrieved_output\n",
    "\n",
    "# Function to save generated code to a single file\n",
    "def save_code_to_file(query, file_path):\n",
    "    generated_code = generate_mcal_code(query)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(\"Generated C and H Code for \" + query + \":\\n\")\n",
    "        file.write(generated_code)\n",
    "\n",
    "# Test the code generation and save to file for the SPI driver\n",
    "query = \"Generate C and H code for CAN driver\"\n",
    "file_path = \"CAN_driver_code.txt\"\n",
    "save_code_to_file(query, file_path)\n",
    "\n",
    "print(\"Generated code saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
